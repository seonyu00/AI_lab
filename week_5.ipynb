{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from IPython.display import Audio\n",
        "\n",
        "# GPU 사용 가능 여부 확인\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# 음성으로 변환할 텍스트\n",
        "text = \"Hello, I'm a student studying computer science.\"\n",
        "\n",
        "# 1. WaveRNN Vocoder 사용\n",
        "# WaveRNN Vocoder는 별도의 OS 제약 사항이 없습니다.\n",
        "print(\"Generating speech with WaveRNN Vocoder...\")\n",
        "bundle_wavernn = torchaudio.pipelines.TACOTRON2_WAVERNN_PHONE_LJSPEECH\n",
        "processor_wavernn = bundle_wavernn.get_text_processor()\n",
        "tacotron2_wavernn = bundle_wavernn.get_tacotron2().to(device)\n",
        "vocoder_wavernn = bundle_wavernn.get_vocoder().to(device)\n",
        "\n",
        "with torch.inference_mode():\n",
        "    processed_wavernn, lengths_wavernn = processor_wavernn(text)\n",
        "    processed_wavernn = processed_wavernn.to(device)\n",
        "    lengths_wavernn = lengths_wavernn.to(device)\n",
        "    spec_wavernn, spec_lengths_wavernn, _ = tacotron2_wavernn.infer(processed_wavernn, lengths_wavernn)\n",
        "    waveforms_wavernn, lengths_wavernn = vocoder_wavernn(spec_wavernn, spec_lengths_wavernn)\n",
        "\n",
        "# 생성된 음성 파일 저장\n",
        "torchaudio.save(\"wavernn_output.wav\", waveforms_wavernn[0:1].cpu(), sample_rate=vocoder_wavernn.sample_rate)\n",
        "print(\"WaveRNN output saved as wavernn_output.wav\")\n",
        "\n",
        "\n",
        "# 2. Griffin-Lim Vocoder 사용\n",
        "# Griffin-Lim Vocoder는 별도의 OS 제약 사항이 없습니다.\n",
        "print(\"\\nGenerating speech with Griffin-Lim Vocoder...\")\n",
        "bundle_griffinlim = torchaudio.pipelines.TACOTRON2_GRIFFINLIM_PHONE_LJSPEECH\n",
        "processor_griffinlim = bundle_griffinlim.get_text_processor()\n",
        "tacotron2_griffinlim = bundle_griffinlim.get_tacotron2().to(device)\n",
        "vocoder_griffinlim = bundle_griffinlim.get_vocoder().to(device)\n",
        "\n",
        "with torch.inference_mode():\n",
        "    processed_griffinlim, lengths_griffinlim = processor_griffinlim(text)\n",
        "    processed_griffinlim = processed_griffinlim.to(device)\n",
        "    lengths_griffinlim = lengths_griffinlim.to(device)\n",
        "    spec_griffinlim, spec_lengths_griffinlim, _ = tacotron2_griffinlim.infer(processed_griffinlim, lengths_griffinlim)\n",
        "    waveforms_griffinlim, lengths_griffinlim = vocoder_griffinlim(spec_griffinlim, spec_lengths_griffinlim)\n",
        "\n",
        "# 생성된 음성 파일 저장\n",
        "torchaudio.save(\"griffinlim_output.wav\", waveforms_griffinlim[0:1].cpu(), sample_rate=vocoder_griffinlim.sample_rate)\n",
        "print(\"Griffin-Lim output saved as griffinlim_output.wav\")\n",
        "\n",
        "\n",
        "# 3. Waveglow Vocoder 사용\n",
        "# Waveglow Vocoder는 Nvidia에서 개발하였으며, 별도의 OS 제약 사항이 없습니다.\n",
        "print(\"\\nGenerating speech with Waveglow Vocoder...\")\n",
        "waveglow = torch.hub.load(\n",
        "    \"NVIDIA/DeepLearningExamples:torchhub\",\n",
        "    \"nvidia_waveglow\",\n",
        "    model_math=\"fp32\",\n",
        "    pretrained=True,\n",
        "    trust_repo=True,\n",
        ")\n",
        "waveglow = waveglow.remove_weightnorm(waveglow)\n",
        "waveglow = waveglow.to(device)\n",
        "waveglow.eval()\n",
        "\n",
        "# Griffin-Lim에서 생성된 spectrogram을 사용합니다.\n",
        "with torch.no_grad():\n",
        "    waveforms_waveglow = waveglow.infer(spec_griffinlim)\n",
        "\n",
        "# 생성된 음성 파일 저장\n",
        "torchaudio.save(\"waveglow_output.wav\", waveforms_waveglow[0:1].cpu(), sample_rate=22050)\n",
        "print(\"Waveglow output saved as waveglow_output.wav\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdOgeo5x9T_e",
        "outputId": "7feffba7-9f52-41ec-e67a-8ced33046fcf"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating speech with WaveRNN Vocoder...\n",
            "Downloading: \"https://download.pytorch.org/torchaudio/models/tacotron2_english_phonemes_1500_epochs_wavernn_ljspeech.pth\" to /root/.cache/torch/hub/checkpoints/tacotron2_english_phonemes_1500_epochs_wavernn_ljspeech.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 107M/107M [00:02<00:00, 49.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/torchaudio/models/wavernn_10k_epochs_8bits_ljspeech.pth\" to /root/.cache/torch/hub/checkpoints/wavernn_10k_epochs_8bits_ljspeech.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16.7M/16.7M [00:00<00:00, 26.0MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:337: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.save_with_torchcodec` under the hood. Some parameters like format, encoding, bits_per_sample, buffer_size, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's encoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.encoders.AudioEncoder\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:247: UserWarning: torio.io._streaming_media_encoder.StreamingMediaEncoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamWriter(uri, format=muxer, buffer_size=buffer_size)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WaveRNN output saved as wavernn_output.wav\n",
            "\n",
            "Generating speech with Griffin-Lim Vocoder...\n",
            "Downloading: \"https://download.pytorch.org/torchaudio/models/tacotron2_english_phonemes_1500_epochs_ljspeech.pth\" to /root/.cache/torch/hub/checkpoints/tacotron2_english_phonemes_1500_epochs_ljspeech.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 107M/107M [00:02<00:00, 47.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Griffin-Lim output saved as griffinlim_output.wav\n",
            "\n",
            "Generating speech with Waveglow Vocoder...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n",
            "/root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/common.py:13: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
            "  warnings.warn(\n",
            "/root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/efficientnet.py:17: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
            "  warnings.warn(\n",
            "Downloading checkpoint from https://api.ngc.nvidia.com/v2/models/nvidia/waveglow_ckpt_fp32/versions/19.09.0/files/nvidia_waveglowpyt_fp32_20190427\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waveglow output saved as waveglow_output.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Jf8gmYu--LHa"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}